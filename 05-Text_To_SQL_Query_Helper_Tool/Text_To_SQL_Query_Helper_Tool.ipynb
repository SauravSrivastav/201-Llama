
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformers is an open-source framework for deep learning created by Hugging Face.\n",
    "# It provides APIs and tools to download state-of-the-art pre-trained models and further tune them to maximize performance.\n",
    "# These models support common tasks in different modalities, such as natural language processing, computer vision, audio, and multi-modal applications.\n",
    "# Using pretrained models can reduce your compute costs, carbon footprint,\n",
    "# and save you the time and resources required to train a model from scratch.\n",
    "\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "# https://huggingface.co/docs/hub/index\n",
    "\n",
    "# Accelerate library to help users easily train a ðŸ¤— Transformers model on any type of distributed setup,\n",
    "# whether it is multiple GPU's on one machine or multiple GPU's across several machines.\n",
    "\n",
    "!pip install -q transformers langchain huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to login to Hugging Face to have access to their inference API.\n",
    "# This step requires a free Hugging Face token.\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class provides functionality related to Hugging Face Transformers pipelines .\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# This line imports the AutoTokenizer class from the transformers library.\n",
    "# The AutoTokenizer class is used to load tokenizers for various pre-trained language models available in the Hugging Face model hub.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This line imports the entire transformers library, which is a popular library developed by\n",
    "# Hugging Face for working with various transformer-based models in natural language processing (NLP),\n",
    "# including both models and tokenizers.\n",
    "import transformers\n",
    "\n",
    "# This line imports the torch library, which is the primary library used for deep learning and tensor computations in PyTorch.\n",
    "import torch\n",
    "\n",
    "# Model name that we want to use\n",
    "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Set up text generation pipeline\n",
    "pipeline = transformers.pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HuggingFacePipeline' class creates a custom pipeline for text generation, and we are passing\n",
    "# the pipeline that we defined earlier along with some model-specific keyword arguments - temperature here.\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "             Create a SQL query snippet using the below text:\n",
    "              ```{text}```\n",
    "              Just SQL query:\n",
    "           \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "text = \"\"\" Extract all the unique values from column \"age\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.run(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
